{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "99eec80c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import matplotlib.pyplot as plt\n",
    "import stumpy\n",
    "import pandas as pd\n",
    "from scipy import signal\n",
    "from sklearn.linear_model import Ridge\n",
    "from scipy.spatial import distance\n",
    "from fatf.utils.kernels import exponential_kernel \n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "\n",
    "from Utils.explanations import LIMESegment, NEVES, LEFTIST, NNSegment, RBP, background_perturb\n",
    "from  Utils.data import loadUCRDataID\n",
    "from  Utils.models import *\n",
    "from  Utils.metrics import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b637baf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset_map = [('Coffee', 0),\n",
    "#                 ('Strawberry', 1),\n",
    "#                    ('GunPointOldVersusYoung', 2),\n",
    "#                    ('HandOutlines', 3),\n",
    "#                     ('yoga', 4),\n",
    "#                     ('ECG200', 5),\n",
    "#                     ('GunPointMaleVersusFemale', 6),\n",
    "#                     ('DodgerLoopGame', 7),\n",
    "#                     ('Chinatown', 8),\n",
    "#                     ('FreezerSmallTrain', 9),\n",
    "#                     ('HouseTwenty', 10),\n",
    "#                     ('WormsTwoClass', 11)\n",
    "#                     ]\n",
    "\n",
    "dataset_map = [('ECG200', 0)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e63b00dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading train / test dataset :  ../Data/Coffee_TRAIN ../Data/Coffee_TEST\n"
     ]
    }
   ],
   "source": [
    "datasets = {}\n",
    "for data_idx in dataset_map:\n",
    "    datasets[data_idx[0]] = loadUCRDataID(data_idx[1])\n",
    "    # print(loadUCRDataID(data_idx[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7b446e2a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Datasets['Coffee'] : Train x, Train y, Test x, Test y\\n\\nTrain x : N x T x 1\\nTrain y: N x 1 \\nTest x: N' x T x 1\\nTest y: N' x 2\""
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"Datasets['Coffee'] : Train x, Train y, Test x, Test y\n",
    "\n",
    "Train x : N x T x 1\n",
    "Train y: N x 1 \n",
    "Test x: N' x T x 1\n",
    "Test y: N' x 2\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "da486949",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(28, 286, 1)\n"
     ]
    }
   ],
   "source": [
    "for data_idx in datasets.keys():\n",
    "    print(datasets[data_idx][0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "765b87a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZES = [8, 64, 32, 64, 64, 16, 16, 4, 4, 4, 8, 16]\n",
    "WINDOW_SIZES = [20, 20, 10, 100, 50, 10, 10, 20, 3, 10, 200, 100]\n",
    "CPS = [5, 5, 4, 8, 5, 3, 4, 5, 2, 5, 8, 6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f27bc322",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ECG200\n",
      "Epoch 1/2\n",
      "3/3 [==============================] - 1s 81ms/step - loss: 0.6522 - sparse_categorical_accuracy: 0.6364 - val_loss: 0.7302 - val_sparse_categorical_accuracy: 0.0000e+00\n",
      "Epoch 2/2\n",
      "3/3 [==============================] - 0s 15ms/step - loss: 0.6447 - sparse_categorical_accuracy: 0.6364 - val_loss: 0.7222 - val_sparse_categorical_accuracy: 0.0000e+00\n",
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_2 (InputLayer)            [(None, 286, 1)]     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "permute (Permute)               (None, 1, 286)       0           input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_3 (Conv1D)               (None, 1, 128)       292992      permute[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_3 (BatchNor (None, 1, 128)       512         conv1d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation (Activation)         (None, 1, 128)       0           batch_normalization_3[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_4 (Conv1D)               (None, 1, 256)       164096      activation[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_4 (BatchNor (None, 1, 256)       1024        conv1d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_1 (Activation)       (None, 1, 256)       0           batch_normalization_4[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_5 (Conv1D)               (None, 1, 128)       98432       activation_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_5 (BatchNor (None, 1, 128)       512         conv1d_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "lstm (LSTM)                     (None, 8)            320         input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_2 (Activation)       (None, 1, 128)       0           batch_normalization_5[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "dropout (Dropout)               (None, 8)            0           lstm[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_1 (Glo (None, 128)          0           activation_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "concatenate (Concatenate)       (None, 136)          0           dropout[0][0]                    \n",
      "                                                                 global_average_pooling1d_1[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 2)            274         concatenate[0][0]                \n",
      "==================================================================================================\n",
      "Total params: 558,162\n",
      "Trainable params: 557,138\n",
      "Non-trainable params: 1,024\n",
      "__________________________________________________________________________________________________\n",
      "Epoch 1/2\n",
      "4/4 - 2s - loss: 0.5725 - accuracy: 0.6786 - val_loss: 0.6785 - val_accuracy: 0.5357\n",
      "Epoch 2/2\n",
      "4/4 - 0s - loss: 0.1749 - accuracy: 0.9643 - val_loss: 0.6627 - val_accuracy: 0.5357\n"
     ]
    }
   ],
   "source": [
    "models = ['knn','cnn','lstmfcn']\n",
    "trained_models = {}\n",
    "i = 0\n",
    "for data_idx in datasets.keys():\n",
    "    print(data_idx)\n",
    "    trained_models[data_idx] = {}\n",
    "    trained_models[data_idx]['knn'] = train_KNN_model(datasets[data_idx][0],datasets[data_idx][1])\n",
    "    model_cnn = make_CNN_model(datasets[data_idx][0].shape[1:])\n",
    "    trained_models[data_idx]['cnn'] = train_CNN_model(model_cnn,\n",
    "                                                      datasets[data_idx][0],\n",
    "                                                      datasets[data_idx][1],\n",
    "                                                      epochs=2,\n",
    "                                                      batch_size=BATCH_SIZES[i])[0]\n",
    "    model_lstmfcn = make_LSTMFCN_model(datasets[data_idx][0].shape[1])\n",
    "    trained_models[data_idx]['lstmfcn'] = train_LSTMFCN_model(model_lstmfcn,\n",
    "                                                      datasets[data_idx][0],\n",
    "                                                      datasets[data_idx][1],\n",
    "                                                      datasets[data_idx][2],\n",
    "                                                      datasets[data_idx][3],\n",
    "                                                      epochs=2,\n",
    "                                                      batch_size=BATCH_SIZES[i])\n",
    "    i = i + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "de21db45",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "EOF while scanning triple-quoted string literal (3663073547.py, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"/var/folders/wr/6tlhy1dx1qn8mdpd33lflg3m0000gn/T/ipykernel_70161/3663073547.py\"\u001b[0;36m, line \u001b[0;32m2\u001b[0m\n\u001b[0;31m    ## model_CNN/LSTMFCN shape  : N x T x 1\u001b[0m\n\u001b[0m                                           \n^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m EOF while scanning triple-quoted string literal\n"
     ]
    }
   ],
   "source": [
    "\"\"\"model_KNN shape : N x T \n",
    "## model_CNN/LSTMFCN shape  : N x T x 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a5bf44f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_TYPES = ['class','proba','proba']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f5859608",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reshaper(x,j):\n",
    "    if j == 0:\n",
    "        return x.reshape(x.shape[0])\n",
    "    else:\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "3954ca99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing explanations for: ECG200\n",
      "\n",
      "processing explanations for: knn\n",
      "\n",
      "{'ECG200': {'knn': {'LS': [(array([ 0.00600599, -0.07204392, -0.17219056, -0.03896472,  0.23288959,\n",
      "       -0.00886859]), [0, 37, 142, 174, 195, 242, -1]), (array([0., 0., 0., 0., 0., 0.]), [0, 141, 174, 193, 242, 259, -1])]}}}\n",
      "processing explanations for: cnn\n",
      "\n",
      "{'ECG200': {'knn': {'LS': [(array([ 0.00600599, -0.07204392, -0.17219056, -0.03896472,  0.23288959,\n",
      "       -0.00886859]), [0, 37, 142, 174, 195, 242, -1]), (array([0., 0., 0., 0., 0., 0.]), [0, 141, 174, 193, 242, 259, -1])], 'LF': [(array([-0.06807765,  0.08106236, -0.09477102, -0.18667103, -0.10339933,\n",
      "        0.07300338,  0.07141118,  0.06004252, -0.2511436 , -0.07807861,\n",
      "       -0.01523657]), [0, 28, 56, 84, 112, 140, 168, 196, 224, 252, 280]), (array([ 0.04295993,  0.02313618, -0.03167679, -0.01300372, -0.1022597 ,\n",
      "        0.07791388, -0.00874422, -0.15479569, -0.23350268, -0.0141455 ,\n",
      "       -0.0098246 ]), [0, 28, 56, 84, 112, 140, 168, 196, 224, 252, 280])]}, 'cnn': {'LS': [(array([0., 0., 0., 0., 0., 0.]), [0, 37, 142, 174, 195, 242, -1]), (array([0., 0., 0., 0., 0., 0.]), [0, 141, 174, 193, 242, 259, -1])]}}}\n",
      "processing explanations for: lstmfcn\n",
      "\n",
      "{'ECG200': {'knn': {'LS': [(array([ 0.00600599, -0.07204392, -0.17219056, -0.03896472,  0.23288959,\n",
      "       -0.00886859]), [0, 37, 142, 174, 195, 242, -1]), (array([0., 0., 0., 0., 0., 0.]), [0, 141, 174, 193, 242, 259, -1])], 'LF': [(array([-0.06807765,  0.08106236, -0.09477102, -0.18667103, -0.10339933,\n",
      "        0.07300338,  0.07141118,  0.06004252, -0.2511436 , -0.07807861,\n",
      "       -0.01523657]), [0, 28, 56, 84, 112, 140, 168, 196, 224, 252, 280]), (array([ 0.04295993,  0.02313618, -0.03167679, -0.01300372, -0.1022597 ,\n",
      "        0.07791388, -0.00874422, -0.15479569, -0.23350268, -0.0141455 ,\n",
      "       -0.0098246 ]), [0, 28, 56, 84, 112, 140, 168, 196, 224, 252, 280])]}, 'cnn': {'LS': [(array([0., 0., 0., 0., 0., 0.]), [0, 37, 142, 174, 195, 242, -1]), (array([0., 0., 0., 0., 0., 0.]), [0, 141, 174, 193, 242, 259, -1])], 'LF': [(array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]), [0, 28, 56, 84, 112, 140, 168, 196, 224, 252, 280]), (array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]), [0, 28, 56, 84, 112, 140, 168, 196, 224, 252, 280])]}, 'lstmfcn': {'LS': [(array([0., 0., 0., 0., 0., 0.]), [0, 37, 142, 174, 195, 242, -1]), (array([0., 0., 0., 0., 0., 0.]), [0, 141, 174, 193, 242, 259, -1])]}}}\n"
     ]
    }
   ],
   "source": [
    "explanations = {}\n",
    "i = 0 \n",
    "noisy_explanations = {} # For Robustness later\n",
    "for data_idx in datasets.keys():\n",
    "    print('processing explanations for: ' + str(data_idx) + '\\n')\n",
    "    explanations[data_idx] = {}\n",
    "    noisy_explanations[data_idx] = {}\n",
    "    j = 0\n",
    "    for model_idx in trained_models[data_idx].keys():\n",
    "        print('processing explanations for: ' + str(model_idx) + '\\n')\n",
    "        explanations[data_idx][model_idx] = {}\n",
    "        noisy_explanations[data_idx][model_idx] = {}\n",
    "        explanation_set = datasets[data_idx][2][0:2]\n",
    "        explanations[data_idx][model_idx]['LS'] = [LIMESegment(reshaper(x,j), trained_models[data_idx][model_idx], model_type=MODEL_TYPES[j], window_size=WINDOW_SIZES[i], cp=CPS[i]) for x in explanation_set]\n",
    "        print(explanations)\n",
    "        # explanations[data_idx][model_idx]['N'] = [NEVES(x, trained_models[data_idx][model_idx], datasets[data_idx][0], model_type=MODEL_TYPES[j]) for x in explanation_set]\n",
    "        explanations[data_idx][model_idx]['LF'] = [LEFTIST(reshaper(x,j), trained_models[data_idx][model_idx], datasets[data_idx][0], model_type=MODEL_TYPES[j],) for x in explanation_set]\n",
    "        \n",
    "\n",
    "        noisy_set = np.asarray([add_noise(x) for x in explanation_set])\n",
    "        \n",
    "        noisy_explanations[data_idx][model_idx]['LS'] = [LIMESegment(reshaper(x,j), trained_models[data_idx][model_idx], model_type=MODEL_TYPES[j], window_size=WINDOW_SIZES[i], cp=CPS[i]) for x in noisy_set]\n",
    "        # noisy_explanations[data_idx][model_idx]['N'] = [NEVES(x, trained_models[data_idx][model_idx], datasets[data_idx][0], model_type=MODEL_TYPES[j]) for x in noisy_set]\n",
    "        noisy_explanations[data_idx][model_idx]['LF'] = [LEFTIST(reshaper(x,j), trained_models[data_idx][model_idx], datasets[data_idx][0], model_type=MODEL_TYPES[j],) for x in noisy_set]\n",
    "        \n",
    "        j = j + 1\n",
    "    i = i + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "17a1ba78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(array([ 0.15642081, -0.04147744, -0.2829171 , -0.04823439,  0.34049784,\n",
      "        0.0258209 ]), [0, 37, 142, 174, 195, 242, -1]), (array([0., 0., 0., 0., 0., 0.]), [0, 141, 174, 193, 242, 259, -1])]\n",
      "[(array([ 0.15642081, -0.04147744, -0.2829171 , -0.04823439,  0.34049784,\n",
      "        0.0258209 ]), [0, 37, 142, 174, 195, 242, -1]), (array([0., 0., 0., 0., 0., 0.]), [0, 141, 174, 193, 242, 259, -1])]\n",
      "[(array([-0.04649363, -0.08004188, -0.02418995,  0.04415766,  0.0242685 ,\n",
      "       -0.07512611, -0.07568266,  0.0609746 , -0.0672467 , -0.00990373,\n",
      "        0.03806916]), [0, 28, 56, 84, 112, 140, 168, 196, 224, 252, 280]), (array([-0.16573141, -0.00049511, -0.03258302, -0.0578341 , -0.13982137,\n",
      "       -0.13128767, -0.0264127 , -0.07383631, -0.04282645,  0.00192508,\n",
      "        0.05782108]), [0, 28, 56, 84, 112, 140, 168, 196, 224, 252, 280])]\n",
      "[(array([-0.04649363, -0.08004188, -0.02418995,  0.04415766,  0.0242685 ,\n",
      "       -0.07512611, -0.07568266,  0.0609746 , -0.0672467 , -0.00990373,\n",
      "        0.03806916]), [0, 28, 56, 84, 112, 140, 168, 196, 224, 252, 280]), (array([-0.16573141, -0.00049511, -0.03258302, -0.0578341 , -0.13982137,\n",
      "       -0.13128767, -0.0264127 , -0.07383631, -0.04282645,  0.00192508,\n",
      "        0.05782108]), [0, 28, 56, 84, 112, 140, 168, 196, 224, 252, 280])]\n",
      "[(array([0., 0., 0., 0., 0., 0.]), [0, 37, 142, 174, 195, 242, -1]), (array([0., 0., 0., 0., 0., 0.]), [0, 141, 174, 193, 242, 259, -1])]\n",
      "[(array([0., 0., 0., 0., 0., 0.]), [0, 37, 142, 174, 195, 242, -1]), (array([0., 0., 0., 0., 0., 0.]), [0, 141, 174, 193, 242, 259, -1])]\n",
      "[(array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]), [0, 28, 56, 84, 112, 140, 168, 196, 224, 252, 280]), (array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]), [0, 28, 56, 84, 112, 140, 168, 196, 224, 252, 280])]\n",
      "[(array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]), [0, 28, 56, 84, 112, 140, 168, 196, 224, 252, 280]), (array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]), [0, 28, 56, 84, 112, 140, 168, 196, 224, 252, 280])]\n",
      "[(array([0., 0., 0., 0., 0., 0.]), [0, 37, 142, 174, 195, 242, -1]), (array([0., 0., 0., 0., 0., 0.]), [0, 141, 174, 193, 242, 259, -1])]\n",
      "[(array([0., 0., 0., 0., 0., 0.]), [0, 37, 142, 174, 195, 242, -1]), (array([0., 0., 0., 0., 0., 0.]), [0, 141, 174, 193, 242, 259, -1])]\n",
      "[(array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]), [0, 28, 56, 84, 112, 140, 168, 196, 224, 252, 280]), (array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]), [0, 28, 56, 84, 112, 140, 168, 196, 224, 252, 280])]\n",
      "[(array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]), [0, 28, 56, 84, 112, 140, 168, 196, 224, 252, 280]), (array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]), [0, 28, 56, 84, 112, 140, 168, 196, 224, 252, 280])]\n"
     ]
    }
   ],
   "source": [
    "evaluation_metrics = {}\n",
    "for data_idx in datasets.keys():\n",
    "    evaluation_metrics[data_idx] = {}\n",
    "    j = 0 \n",
    "    for model_idx in trained_models[data_idx].keys():\n",
    "        evaluation_metrics[data_idx][model_idx] = {}\n",
    "        for explanation_idx in explanations[data_idx][model_idx].keys():\n",
    "            evaluation_metrics[data_idx][model_idx][explanation_idx] = {}\n",
    "            # Robustness\n",
    "            evaluation_metrics[data_idx][model_idx][explanation_idx]['Robustness'] = robustness(explanations[data_idx][model_idx][explanation_idx],\n",
    "                                                                                         noisy_explanations[data_idx][model_idx][explanation_idx])\n",
    "            explanation_set = datasets[data_idx][2][0:2]\n",
    "            explanation_labels = datasets[data_idx][3][0:2]\n",
    "            if j == 0:\n",
    "                explanation_predictions = trained_models[data_idx][model_idx].predict(explanation_set.reshape(explanation_set.shape[:2]))\n",
    "            \n",
    "            else:\n",
    "                explanation_predictions = trained_models[data_idx][model_idx].predict(explanation_set)\n",
    "                \n",
    "                # Faithfulness\n",
    "            evaluation_metrics[data_idx][model_idx][explanation_idx]['Faithfulness'] = faithfulness(explanations[data_idx][model_idx][explanation_idx],explanation_set,explanation_labels,explanation_predictions,trained_models[data_idx][model_idx],model_type=MODEL_TYPES[j])\n",
    "        j+=1\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f336239d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97e12787",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9b18870",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
